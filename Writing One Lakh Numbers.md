#Streams

## ğŸ¢ Using `fs.appendFile()`

ASYNCHRONIOUS VERSION : This version not able write all numbers correctly in order
```js
console.time()

for (let i = 1; i <= 10000; i++) {
Â  Â  if (i == 1) fs.writeFile('couter.txt', `${i},`, () => { })
Â  Â  else fs.appendFile('couter.txt', `${i},`, () => {
Â  Â  Â  Â  if (i === 10000) {
Â  Â  Â  Â  Â  Â  console.timeEnd(); //default: 3.364s
Â  Â  Â  Â  }
Â  Â  })
}
```

SYNCHRONIOUS VERSION : This version able to write in correct order
```js
import fs from "fs";
console.time()

for (let i = 1; i <= 100000; i++) {
Â  Â  if (i == 1) fs.writeFileSync('couter.txt', `${i},`)
Â  Â  else fs.appendFileSync('couter.txt', `${i},`)
}

console.timeEnd() //default: 1:10.450 (m:ss.mmm)
```


### âŒ Performance

- Takes a lot of time (~23 seconds for 1 lakh entries)
- Loads the **entire string into memory** before writing

### ğŸ§  Reason

- `appendFile()` writes all data at once
- Blocks RAM and CPU due to large string allocation

---

## âš¡ Using `fs.createWriteStream()`

  
```js

import fs from "fs";

console.time();
const writeStream = fs.createWriteStream("streamedNumber.txt");
for (let i = 1; i <= 100000; i++) {
Â  writeStream.write(`${i}\n`);
}

writeStream.end();

writeStream.on("finish", () => {
Â  console.timeEnd(); // default: 889.575ms
});

```

  

### âœ… Performance

- **Super fast** (~889 ms for 1 lakh numbers)
- **Efficient memory usage**
- Writes **data chunk by chunk**
- Handles **backpressure** behind the scenes

---

## ğŸ“Š Comparison Table

| Method                   |    Time (Approx.)     | Memory Efficient | Best Use Case            |
| :----------------------- | :-------------------: | :--------------: | :----------------------- |
| `fs.appendFile()`        | âŒ Very Slow (~1minut) |       âŒ No       | Small / quick writes     |
| `fs.createWriteStream()` | âœ… Very Fast (~889ms)  |      âœ… Yes       | Large-scale data writing |

---

## ğŸ§© Key Takeaways

- Use **streams** (`fs.createWriteStream`) for **large files** like logs, analytics, or exports. Â 
- Avoid loading large data fully into memory â€” **stream it instead**. Â 
- Streams use **internal buffers** for optimal performance. Â 

---
### ğŸ’¡ Tip

If your program needs to handle **massive file operations**, prefer:
- `fs.createReadStream()` + `fs.createWriteStream()`
- Use **piping** for efficient data transfer between streams.

---

**Conclusion:** Â 

> Streaming is the way to go for large-scale file operations in Node.js. ğŸ”¥


## [[Why Streams are so Fast ]]